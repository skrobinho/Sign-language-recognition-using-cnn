{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign language recognition using deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON, LinearAlgebra\n",
    "using CSV\n",
    "using Images\n",
    "using GMT\n",
    "using StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "sign_mnist_test=CSV.read(\"sign_mnist_test.csv\")\n",
    "sign_mnist_train=CSV.read(\"sign_mnist_train.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the occurrences of individual letters in training and test sets\n",
    "countmap(sign_mnist_test[:label])\n",
    "countmap(sign_mnist_train[:label]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test=sign_mnist_test[:,1]\n",
    "y_train=sign_mnist_train[:,1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test=sign_mnist_test[:,2:785]\n",
    "x_train = sign_mnist_train[:,2:785];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Converting colors to grayscale\n",
    "x_test=x_test./255\n",
    "x_train=x_train./255;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting array [27455:784] to array [27455:[28:28]]\n",
    "x_train1 = [reshape(Array{Float64}(x_train)[i,:], (28, 28)) for i in 1:size(Array{Float64}(x_train))[1]]\n",
    "Gray.(x_train1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1 = [reshape(Array{Float64}(x_test)[i,:], (28, 28)) for i in 1:size(Array{Float64}(x_test))[1]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that partitions the set into batch_size partitions\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:24)\n",
    "    return (X_batch, Y_batch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100;\n",
    "batch_size = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partitioning the set\n",
    "mb_idxs = partition(1:length(x_train1), batch_size)\n",
    "train_set = [make_minibatch(x_train1, y_train, i) for i in mb_idxs];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "model = Chain(\n",
    "    Conv((5, 5), 1=>6, stride=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "    Conv((5, 5), 6=>16, stride=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "    Conv((5, 5), 16=>200, pad=(1,1), relu),\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(800, 25),\n",
    "    softmax,\n",
    ");\n",
    "\n",
    "#Loss function\n",
    "function loss(x, y)\n",
    "    x_aug = x .+ 0.1f0*randn(eltype(x), size(x))\n",
    "    y_hat = model(x_aug)\n",
    "    return crossentropy(y_hat, y)\n",
    "end\n",
    "\n",
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "opt = ADAM();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = make_minibatch(x_test1, y_test, 1:length(x_test1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "@info(\"Beginning training loop...\")\n",
    "best_acc = 0.0\n",
    "last_improvement = 0\n",
    "for epoch = 1:epochs\n",
    "    global best_acc, last_improvement\n",
    "    Flux.train!(loss, params(model), train_set, opt)\n",
    "    acc = accuracy(test_set[1],test_set[2])\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch, acc))\n",
    "    if acc >= 0.90\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of 90%\")\n",
    "        break\n",
    "    end\n",
    "    if acc <= 0.05\n",
    "        @info(\" -> Best accurancy is: $(best_acc)\")\n",
    "        break\n",
    "    end\n",
    "    if acc >= best_acc\n",
    "        @info(\" -> New best accuracy! Saving model out to sign_lang_MNIST_conv.bson\")\n",
    "        BSON.@save \"sign_lang_MNIST_conv.bson\" model epoch acc\n",
    "        best_acc = acc\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 5 && opt.eta > 1e-6\n",
    "        opt.eta /= 10.0\n",
    "        @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 10\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that partitions the set into batch_size partitions\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:24)\n",
    "    return (X_batch, Y_batch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500;\n",
    "batch_size = 128;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partitioning the set\n",
    "mb_idxs = partition(1:length(x_train1), batch_size)\n",
    "train_set = [make_minibatch(x_train1, y_train, i) for i in mb_idxs];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = make_minibatch(x_test1, y_test, 1:length(x_test1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "mlp =   Chain(x -> reshape(x, :, size(x, 4)),\n",
    "        Dense(784,256, relu),\n",
    "        Dropout(0.4),\n",
    "        Dense(256,128, relu),\n",
    "        Dropout(0.4),\n",
    "        Dense(128,25, relu),\n",
    "        softmax,\n",
    ")\n",
    "\n",
    "Î± = 2.0f-6\n",
    "\n",
    "#Loss function\n",
    "function mlp_loss(x, y)\n",
    "    y_hat = mlp(x)\n",
    "    return crossentropy(y_hat, y) \n",
    "end\n",
    "mlp_accuracy(x, y) = mean(onecold(mlp(x)) .== onecold(y))\n",
    "\n",
    "opt = ADAM(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "@info(\"Beginning training loop...\")\n",
    "best_acc = 0.0\n",
    "last_improvement = 0\n",
    "for epoch = 1:epochs\n",
    "    global best_acc, last_improvement\n",
    "    Flux.train!(mlp_loss, params(mlp), train_set, opt)\n",
    "    acc = mlp_accuracy(test_set[1],test_set[2])\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch, acc))\n",
    "    if acc >= 0.91\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of 90%\")\n",
    "        break\n",
    "    end\n",
    "    if acc >= best_acc\n",
    "        @info(\" -> New best accuracy! Saving model out to fashionMNIST_mlp.bson\")\n",
    "        BSON.@save \"fashionMNIST_mlp.bson\" mlp epoch acc\n",
    "        best_acc = acc\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 5 && opt.eta > 1e-20\n",
    "        opt.eta /= 10.0\n",
    "        @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 10\n",
    "        @warn(\" -> We're calling this converged. Best acc: $(best_acc)\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
